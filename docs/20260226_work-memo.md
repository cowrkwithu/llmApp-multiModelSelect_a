# 2026-02-26 작업 메모

## 작업 개요
프로젝트 전체 점검 및 현재 시스템 환경(RTX 3090, 24GB VRAM)에 맞춘 수정

## 시스템 환경 확인
- **GPU**: NVIDIA GeForce RTX 3090, 24576 MiB (기존 문서: RTX 2070 8GB → 실제와 불일치)
- **Node.js**: v20.20.0
- **Python**: 3.13.5
- **npm**: 10.8.2
- **Docker**: 실행 중 (기존 다른 프로젝트 컨테이너 존재, 본 프로젝트 컨테이너는 미실행 상태)

## 발견된 문제 및 수정 사항

### 1. TypeScript 타입 에러 수정
- **파일**: `frontend/src/lib/types.ts`
- **문제**: `ChatSSEEvent.type` union 타입에 `"error"`가 누락되어 있었음. 백엔드 `rag_pipeline.py`에서 에러 시 `{"type": "error", "content": ...}`를 전송하고, 프론트엔드 `use-chat.ts:64`에서 `event.type === "error"`를 체크하지만 타입 정의에 없어 TypeScript strict 모드에서 에러 발생
- **수정**: `type: "sources" | "token" | "done"` → `type: "sources" | "token" | "done" | "error"`

### 2. start.sh 로그 디렉토리 누락 수정
- **파일**: `scripts/start.sh`
- **문제**: `$PROJECT_DIR/logs/backend.log` 등에 로그를 기록하지만 `logs/` 디렉토리가 존재하지 않아 스크립트 실행 시 실패
- **수정**: Docker 서비스 시작 전에 `mkdir -p "$PROJECT_DIR/logs"` 추가

### 3. 모델 전환 시 컨테이너 이름 충돌 수정
- **파일**: `backend/app/api/models.py`
- **문제**: `docker compose stop vllm` 후 `docker compose run -d --name vllm-server ...` 실행 시, 중단된 컨테이너가 제거되지 않아 `--name vllm-server` 충돌 발생
- **수정**: `stop`과 `run` 사이에 `docker compose rm -f vllm` 단계 추가. 또한 `cwd` 계산을 변수로 추출하여 중복 제거

### 4. vLLM Docker 설정 최적화 (RTX 3090 대응)
- **파일**: `docker-compose.yml`
- **수정 내용**:
  - `--enforce-eager` 제거: RTX 3090은 Flash Attention 지원하므로 eager mode 불필요. 제거 시 추론 성능 향상
  - `VLLM_MAX_MODEL_LEN` 기본값 2048 → 8192: 24GB VRAM이면 7B Int4 모델(~4-5GB)로 8192 토큰 컨텍스트 충분히 처리 가능
  - `VLLM_GPU_MEMORY_UTILIZATION` 기본값 0.92 → 0.90: 약간의 여유를 두어 OOM 방지

### 5. .env.example 업데이트
- **파일**: `.env.example`
- **수정**: `VLLM_GPU_MEMORY_UTILIZATION` 0.92→0.90, `VLLM_MAX_MODEL_LEN` 2048→8192

### 6. .env 파일 생성
- **파일**: `.env` (신규 생성, .gitignore에 포함되어 있어 커밋 안됨)
- **내용**: `.env.example` 기반으로 RTX 3090에 맞는 설정값으로 생성

### 7. CLAUDE.md 문서 업데이트
- **파일**: `CLAUDE.md`
- **수정**:
  - GPU 정보: `RTX 2070 (8GB VRAM)` → `RTX 3090 (24GB VRAM)`
  - 모델 설명: `8GB VRAM에 맞춰야 함` → `24GB VRAM으로 넉넉한 컨텍스트 길이 가능`

## 빌드 검증 결과
- **TypeScript typecheck**: `tsc --noEmit` 통과 (에러 0)
- **Next.js build**: `next build` 성공, 모든 라우트 정적 생성 완료
  - `/`, `/_not-found`, `/chat`, `/collections`, `/documents`

## 수정 파일 목록

| 파일 | 변경 유형 | 요약 |
|------|-----------|------|
| `frontend/src/lib/types.ts` | 수정 | ChatSSEEvent에 "error" 타입 추가 |
| `scripts/start.sh` | 수정 | logs 디렉토리 자동 생성 |
| `backend/app/api/models.py` | 수정 | 모델 전환 시 컨테이너 rm 추가 |
| `docker-compose.yml` | 수정 | enforce-eager 제거, max_model_len 증가 |
| `.env.example` | 수정 | RTX 3090 기본값 반영 |
| `.env` | 신규 | 로컬 환경설정 파일 생성 |
| `CLAUDE.md` | 수정 | GPU 정보 RTX 3090으로 갱신 |

## 미실행 사항 (추후 필요)
- Docker 컨테이너 실행 및 서비스 통합 테스트
- Backend Python 가상환경 생성 및 의존성 설치
- vLLM 모델 다운로드 (`scripts/download-model.sh`)

---

## 스크립트 환경 적합성 수정

### 사용자 프롬프트
> script 폴더내의 start.sh 파일과 stop.sh 파일도 현재 시스템 환경에 적합하게 설정되어 있는지 확인하고 필요한 부분은 수정해줘.

### 8. start.sh / stop.sh - .env 파일 로딩 추가
- **파일**: `scripts/start.sh`, `scripts/stop.sh`
- **문제**: 스크립트가 `.env` 파일을 읽지 않아 환경변수가 기본값에만 의존. 특히 백엔드는 `backend/` 디렉토리에서 실행되므로, pydantic-settings가 프로젝트 루트의 `.env`를 찾지 못함
- **수정**: 양쪽 스크립트 초반에 `set -a; source "$PROJECT_DIR/.env"; set +a` 추가. `set -a`로 변수를 자동 export하여 모든 자식 프로세스(docker compose, uvicorn, npm)에 전달

### 9. stop.sh - `wait` 명령 제거 및 프로세스 트리 종료 수정
- **파일**: `scripts/stop.sh`
- **문제 1**: `wait "$PID"`는 현재 셸의 자식 프로세스에만 작동. stop.sh는 별도 셸이므로 start.sh에서 시작된 PID에 대해 `wait`가 무효 (즉시 리턴)
- **문제 2**: 프론트엔드 `npm run dev`는 자식 프로세스(next)를 생성하는데, npm PID만 kill하면 next 프로세스가 고아 프로세스로 남음
- **수정**:
  - `wait "$PID"` → `sleep 1`로 교체 (종료 대기)
  - 프론트엔드에 `pkill -P "$PID"` 추가 (자식 프로세스 먼저 종료)
  - PID 파일 기반 종료 후에도 포트 기반 정리를 항상 수행 (고아 프로세스 대비)

### 수정 파일 목록

| 파일 | 변경 유형 | 요약 |
|------|-----------|------|
| `scripts/start.sh` | 수정 | `.env` 로딩 (`set -a/source/set +a`) 추가 |
| `scripts/stop.sh` | 수정 | `.env` 로딩, `wait` 제거, `pkill -P` 자식 프로세스 종료, 포트 기반 정리 항상 수행 |

---

## vLLM → Ollama 마이그레이션

### 사용자 프롬프트
> vLLM Docker 컨테이너를 제거하고 이미 네이티브 설치된 Ollama (v0.15.1)로 마이그레이션. Ollama에 exaone3.5:7.8b, qwen3:14b, gemma3:27b 3개 모델 준비됨. OpenAI 호환 API 사용하여 기존 AsyncOpenAI 클라이언트 재활용.

### 변경 요약
- **LLM 엔진**: vLLM (Docker, port 8030) → Ollama (native, port 11434)
- **모델 관리**: 정적 `models.json` → Ollama `/api/tags` 동적 조회
- **모델 전환**: Docker 컨테이너 재시작 (수 분) → 인메모리 선택값 변경 (즉시)
- **Docker**: vLLM + Qdrant → Qdrant만 사용

### 수정 파일 목록

| 파일 | 변경 유형 | 요약 |
|------|-----------|------|
| `backend/app/core/config.py` | 수정 | vllm_* 설정 제거, ollama_* 설정 추가 (host, port, base_url, api_url) |
| `backend/app/services/ollama_client.py` | 신규 | Ollama OpenAI 클라이언트, 헬스체크, 모델 목록/로드 상태 조회 |
| `backend/app/schemas/models.py` | 수정 | ModelInfo: context_length/languages/description → parameter_size/family, ActiveModelResponse: vllm_status → status |
| `backend/app/api/models.py` | 수정 | subprocess/Docker 제거, Ollama API 동적 조회, 인메모리 모델 선택 |
| `backend/app/services/rag_pipeline.py` | 수정 | import vllm_client → ollama_client |
| `backend/app/api/chat.py` | 수정 | import vllm_client → ollama_client, 에러 메시지 변경 |
| `backend/app/services/vllm_client.py` | 삭제 | ollama_client.py로 대체 |
| `frontend/src/lib/types.ts` | 수정 | ModelInfo/ActiveModel 필드 변경 (vllm_status → status 등) |
| `frontend/src/app/page.tsx` | 수정 | "vLLM Status" → "Ollama Status", vllm_status → status |
| `frontend/src/components/models/model-selector.tsx` | 수정 | vllm_status → status, 모델 표시 형식 변경 |
| `docker-compose.yml` | 수정 | vllm 서비스 전체 제거, backend에 Ollama 연결 설정 |
| `.env` | 수정 | VLLM_* 제거, OLLAMA_* 추가 |
| `.env.example` | 수정 | VLLM_* 제거, OLLAMA_* 추가 |
| `scripts/start.sh` | 수정 | vLLM Docker 시작 → Ollama 상태 확인 |
| `scripts/stop.sh` | 수정 | vLLM 관련 텍스트 변경, Ollama는 중지하지 않음 |
| `scripts/health-check.sh` | 수정 | vLLM 체크 → Ollama 체크 |
| `scripts/download-model.sh` | 수정 | huggingface-cli → ollama pull |
| `docker/vllm/models.json` | 삭제 | Ollama 동적 조회로 대체 |
| `.gitignore` | 수정 | docker/vllm/cache/ 항목 제거 |
| `CLAUDE.md` | 수정 | 아키텍처 설명 전반 업데이트 (vLLM → Ollama) |
| `README.md` | 수정 | 전면 재작성 (아키텍처, 설치 방법, 설정 등) |

---

## 모델 자동 선택 (Unknown 상태 수정)

### 사용자 프롬프트
> dashboard와 chat 화면에 Model상태가 Unknown으로 표시됨. 설치된 LLM 모델들을 선택할 수 있도록 수정

### 원인
- `_selected_model_id`가 서버 시작 시 `None`으로 초기화
- `/active` API가 `model_id: null` 반환 → 프론트엔드에서 모델 미선택 상태

### 해결
- `get_active_model()`에서 `_selected_model_id`가 `None`이고 Ollama healthy이면 설치된 첫 번째 모델을 자동 선택

### 수정 파일 목록

| 파일 | 변경 유형 | 요약 |
|------|-----------|------|
| `backend/app/api/models.py` | 수정 | active 엔드포인트에서 모델 미선택 시 첫 번째 모델 자동 선택 |

### 검증 결과
- `GET /api/v1/models/active` → `{"model_id": "exaone3.5:7.8b", "status": "healthy"}`
- `POST /api/v1/models/switch` → 모델 전환 정상 동작
