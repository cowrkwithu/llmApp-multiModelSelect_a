# 작업 메모 - 2026-02-25

## 프로젝트 개요

**프로젝트명**: llmApp-multiModelSelect_a (Local RAG LLM Application)
**목표**: vLLM 기반 로컬 RAG 애플리케이션을 빈 git 저장소에 처음부터 구축
**작업 환경**: WSL2 Linux, NVIDIA RTX 2070 (8GB VRAM), CUDA 12.3 (Driver 546.12)

---

## 1. 구현 계획 수립 및 승인

### 사용자 프롬프트
> Implement the following plan: (Local RAG LLM Application - Implementation Plan 전문 제공)

### 진행 내용
- 사용자가 상세한 구현 계획서를 제공 (아키텍처, API 설계, 프로젝트 구조, 구현 단계 등 포함)
- 빈 git 저장소 상태 확인 (.git과 docs/ 폴더만 존재)
- 6개의 작업 태스크를 생성하여 체계적 진행 계획 수립:
  1. Create project infrastructure files
  2. Create backend core (config, dependencies, main.py)
  3. Implement backend services
  4. Create API endpoints and schemas
  5. Build Next.js frontend
  6. Create scripts and backend Dockerfile

---

## 2. Phase 1: 인프라 파일 생성

### 생성 파일 목록

| 파일 | 설명 |
|------|------|
| `.gitignore` | Python, Node, Docker, data 디렉토리 등 제외 규칙 |
| `.env.example` | 환경변수 템플릿 (vLLM, Qdrant, Backend, Embedding, RAG 설정) |
| `CLAUDE.md` | 프로젝트 컨텍스트 문서 |
| `docker-compose.yml` | vLLM + Qdrant + Backend(profile:full) 서비스 정의 |
| `docker/vllm/models.json` | 3개 모델 레지스트리 (Qwen2.5-7B, Mistral-7B, Llama-3.1-8B) |

### docker-compose.yml 초기 설정
- vLLM: `vllm/vllm-openai:latest`, port 8030, GPU 85% VRAM
- Qdrant: `qdrant/qdrant:latest`, port 6333/6334
- Backend: profile `full`, port 8020

---

## 3. Phase 2: 백엔드 코어 생성

### 생성 파일 목록
- `backend/requirements.txt` - 전체 Python 의존성 (fastapi, langchain, sentence-transformers 등)
- `backend/app/__init__.py` 및 각 모듈 `__init__.py` (7개)
- `backend/app/core/config.py` - Pydantic Settings 기반 설정 관리
- `backend/app/core/dependencies.py` - Qdrant 클라이언트, 임베딩 모델 싱글톤
- `backend/app/main.py` - FastAPI 앱 (CORS, lifespan, 라우터 등록, /health 엔드포인트)

---

## 4. Phase 3: 백엔드 서비스 구현

### 생성 파일 목록 (7개)

| 파일 | 역할 |
|------|------|
| `services/vllm_client.py` | AsyncOpenAI 클라이언트, vLLM 헬스체크, 로드된 모델 확인 |
| `services/embedding_service.py` | CPU sentence-transformers 임베딩 (E5 query/passage 프리픽스) |
| `services/qdrant_service.py` | 컬렉션 CRUD, 벡터 upsert/검색, document_id 기반 삭제 |
| `services/document_processor.py` | 파일 파싱 → 청킹 → 임베딩 → Qdrant 저장 파이프라인 |
| `services/rag_pipeline.py` | RAG 체인 + SSE 스트리밍 (소스 인용 포함) |
| `parsers/pdf_parser.py, docx_parser.py, txt_parser.py` | 각 포맷별 문서 파서 |
| `utils/text_splitter.py, file_utils.py` | 텍스트 분할, 파일 유틸리티 |

---

## 5. Phase 4: API 엔드포인트 및 스키마 생성

### 스키마 (4개)
- `schemas/models.py` - ModelInfo, ActiveModelResponse, SwitchModelRequest 등
- `schemas/documents.py` - DocumentUploadResponse, UploadResult 등
- `schemas/chat.py` - ChatQueryRequest, ChatNoRagRequest
- `schemas/collections.py` - CollectionCreateRequest, CollectionResponse 등

### API 라우터 (4개)
- `api/models.py` - GET /, GET /active, POST /switch, GET /status
- `api/documents.py` - POST /upload, GET /, DELETE /{id}
- `api/chat.py` - POST /query (SSE), POST /query-no-rag (SSE)
- `api/collections.py` - GET /, POST /, DELETE /{name}

---

## 6. Phase 5: Next.js 프론트엔드 구축

### 사용자 프롬프트
> (Phase 4의 일부로 자동 진행)

### Next.js 프로젝트 초기화

**create-next-app 실행 이슈:**
- 첫 번째 시도: `npx create-next-app@latest` 실행 시 "Would you like to use React Compiler?" 프롬프트에서 대기 상태
- 두 번째 시도: `< /dev/null` 리다이렉트로 시도했으나 동일 문제
- **해결**: `yes "" | npx create-next-app@latest ...` 파이프로 비대화식 실행 성공

### 설정 변경
- `package.json`: dev 스크립트에 `--port 3020` 추가
- 추가 의존성 설치: `@tanstack/react-query`, `zustand`

### 생성 파일 목록

**라이브러리 (2개)**
- `lib/types.ts` - TypeScript 인터페이스 (ModelInfo, Source, ChatSSEEvent 등)
- `lib/api.ts` - API 클라이언트 (models, collections, documents, chat SSE)

**훅 (4개)**
- `hooks/use-models.ts` - useModels, useActiveModel, useSwitchModel
- `hooks/use-collections.ts` - useCollections, useCreateCollection, useDeleteCollection
- `hooks/use-documents.ts` - useDocuments, useUploadDocuments, useDeleteDocument
- `hooks/use-chat.ts` - useChat (SSE 스트리밍 처리, abort 제어)

**컴포넌트 (7개)**
- `components/providers.tsx` - QueryClientProvider 래퍼
- `components/models/model-selector.tsx` - 모델 선택 드롭다운 + 상태 표시
- `components/collections/collection-manager.tsx` - 컬렉션 목록/생성/삭제
- `components/documents/document-uploader.tsx` - 파일 업로드 (multipart)
- `components/documents/document-list.tsx` - 문서 목록 + 삭제
- `components/chat/chat-input.tsx` - 메시지 입력 + 전송/중지 버튼
- `components/chat/chat-messages.tsx` - 채팅 메시지 렌더링
- `components/chat/source-citation.tsx` - 소스 인용 표시

**페이지 (4개)**
- `app/layout.tsx` - 루트 레이아웃 (네비게이션, Providers 래퍼)
- `app/page.tsx` - 대시보드 (상태 카드, 모델 선택기, 퀵 링크)
- `app/chat/page.tsx` - 채팅 인터페이스 (사이드바 + 채팅 영역, RAG 토글)
- `app/documents/page.tsx` - 문서 관리
- `app/collections/page.tsx` - 컬렉션 관리

### 빌드 검증
- `npx next build` 실행 → **성공** (모든 4개 라우트 정적 생성 완료)

---

## 7. Phase 6: 스크립트 및 Dockerfile

### 생성 파일 목록
- `scripts/download-model.sh` - HuggingFace 모델 다운로드 스크립트 (chmod +x)
- `scripts/health-check.sh` - 전체 서비스 헬스체크 스크립트 (chmod +x)
- `backend/Dockerfile` - Python 3.12-slim 기반 프로덕션 이미지

---

## 8. Git 커밋 및 푸시

### 사용자 프롬프트
> commit this

### 진행 내용
- 70개 파일 스테이징 (`.env`, credentials 등 민감 파일 없음 확인)
- `frontend/next-env.d.ts`가 `.gitignore`에 의해 무시됨 → 스테이징에서 제외

### 커밋
```
commit 3538ccf
"Add local RAG LLM application with multi-model selection"
70 files changed, 9135 insertions(+)
```

### 사용자 프롬프트
> push it

### 진행 내용
- `git push -u origin main` → 성공
- Remote: `https://github.com/cowrkwithu/llmApp-multiModelSelect_a.git`

---

## 9. README.md 생성

### 사용자 프롬프트
> 이 프로젝트의 내용, 시스템 아키텍처, 설치방법, 서버 기동을 위한 사용자 명령등을 포함하여 README.md 파일을 생성해줘.

### 진행 내용
- 프로젝트 주요 파일들 (docker-compose.yml, .env.example, requirements.txt, package.json, models.json) 확인 후 작성
- README.md 307줄 생성: 시스템 아키텍처 다이어그램, 설계 결정사항, RAG 파이프라인 흐름, 지원 모델, Prerequisites, 설치 가이드(5단계), 사용법, API 엔드포인트 레퍼런스, 프로젝트 구조, 환경변수 설정 표, Docker Compose 명령어, 기술 스택

### 커밋 및 푸시
```
commit cafafd2
"Add project README with architecture, setup, and API documentation"
1 file changed, 307 insertions(+)
```
- `git push` → 성공

---

## 10. 백엔드 서버 실행

### 사용자 프롬프트
> 백엔드 서버 실행해봐

### 진행 내용
1. 가상환경 생성: `python3 -m venv .venv`
2. 의존성 설치: `pip install -r requirements.txt` (전체 패키지 설치 완료)
3. 서버 실행: `uvicorn app.main:app --host 0.0.0.0 --port 8020 --reload` (백그라운드)
4. 헬스체크: `curl http://localhost:8020/health` → `{"status":"ok"}` 확인

---

## 11. Docker Compose 서비스 기동

### 사용자 프롬프트
> docker compose up -d

### 시도 1: vllm/vllm-openai:latest (실패)

**에러 메시지:**
```
nvidia-container-cli: requirement error: unsatisfied condition: cuda>=12.9,
please update your driver to a newer version, or use an earlier cuda container
```

**원인 분석:**
- 현재 시스템: NVIDIA Driver 546.12, CUDA 12.3 지원
- `vllm/vllm-openai:latest` 이미지가 CUDA 12.9 요구 → 호환 불가

**조치:**
- `docker-compose.yml`에서 이미지를 `vllm/vllm-openai:v0.6.6.post1` (CUDA 12.4 기반)로 변경
- `docker compose down` 후 재기동

### 시도 2: v0.6.6.post1 (컨테이너 기동 성공, 모델 로딩 실패)

**에러 메시지:**
```
total_gpu_memory (8.00GiB) x gpu_memory_utilization (0.85) = 6.80GiB
model weights take 5.20GiB; non_torch_memory takes 0.20GiB;
PyTorch activation peak memory takes 1.42GiB;
the rest of the memory reserved for KV Cache is -0.02GiB.
→ No available memory for the cache blocks.
```

**원인 분석:**
- 8GB VRAM에서 0.85 할당 = 6.80GiB
- 모델(5.20) + 비Torch(0.20) + 활성화 피크(1.42) = 6.82GiB → KV Cache에 -0.02GiB (부족)

**조치:**
- `gpu_memory_utilization`: 0.85 → **0.92** (7.36GiB 할당으로 변경)
- `max-model-len`: 4096 → **2048** (KV Cache 크기 절반으로 축소)
- `docker compose down` 후 재기동

### 시도 3: 조정된 설정 (성공)

**최종 확인 결과:**
```
vLLM:    healthy (Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4 로드 완료)
Qdrant:  healthz check passed
Backend: {"status":"ok"}
```

| Service | Image | Port | Status |
|---------|-------|------|--------|
| vllm-server | vllm/vllm-openai:v0.6.6.post1 | 8030 | Healthy |
| qdrant-server | qdrant/qdrant:latest | 6333 | Healthy |
| backend (local) | uvicorn | 8020 | Running |

---

## 최종 변경사항 요약 (계획 대비)

| 항목 | 원래 계획 | 실제 적용 | 변경 사유 |
|------|----------|----------|----------|
| vLLM 이미지 | `latest` | `v0.6.6.post1` | CUDA 12.3 드라이버 호환성 |
| GPU Memory Util | 0.85 | 0.92 | 8GB VRAM에서 KV Cache 확보 |
| Max Model Len | 4096 | 2048 | 메모리 부족으로 축소 |

---

## 생성된 전체 파일 목록 (70개)

```
.gitignore
.env.example
CLAUDE.md
README.md
docker-compose.yml
docker/vllm/models.json
scripts/download-model.sh
scripts/health-check.sh
backend/Dockerfile
backend/requirements.txt
backend/app/__init__.py
backend/app/main.py
backend/app/core/__init__.py
backend/app/core/config.py
backend/app/core/dependencies.py
backend/app/api/__init__.py
backend/app/api/models.py
backend/app/api/documents.py
backend/app/api/chat.py
backend/app/api/collections.py
backend/app/schemas/__init__.py
backend/app/schemas/models.py
backend/app/schemas/documents.py
backend/app/schemas/chat.py
backend/app/schemas/collections.py
backend/app/services/__init__.py
backend/app/services/vllm_client.py
backend/app/services/embedding_service.py
backend/app/services/qdrant_service.py
backend/app/services/document_processor.py
backend/app/services/rag_pipeline.py
backend/app/parsers/__init__.py
backend/app/parsers/pdf_parser.py
backend/app/parsers/docx_parser.py
backend/app/parsers/txt_parser.py
backend/app/utils/__init__.py
backend/app/utils/text_splitter.py
backend/app/utils/file_utils.py
frontend/package.json
frontend/package-lock.json
frontend/tsconfig.json
frontend/next.config.ts
frontend/postcss.config.mjs
frontend/eslint.config.mjs
frontend/.gitignore
frontend/src/lib/types.ts
frontend/src/lib/api.ts
frontend/src/hooks/use-models.ts
frontend/src/hooks/use-collections.ts
frontend/src/hooks/use-documents.ts
frontend/src/hooks/use-chat.ts
frontend/src/components/providers.tsx
frontend/src/components/models/model-selector.tsx
frontend/src/components/collections/collection-manager.tsx
frontend/src/components/documents/document-uploader.tsx
frontend/src/components/documents/document-list.tsx
frontend/src/components/chat/chat-input.tsx
frontend/src/components/chat/chat-messages.tsx
frontend/src/components/chat/source-citation.tsx
frontend/src/app/layout.tsx
frontend/src/app/page.tsx
frontend/src/app/globals.css
frontend/src/app/favicon.ico
frontend/src/app/chat/page.tsx
frontend/src/app/documents/page.tsx
frontend/src/app/collections/page.tsx
frontend/public/file.svg
frontend/public/globe.svg
frontend/public/next.svg
frontend/public/vercel.svg
frontend/public/window.svg
```

---

## Git 이력

| Commit | Message |
|--------|---------|
| `3538ccf` | Add local RAG LLM application with multi-model selection |
| `cafafd2` | Add project README with architecture, setup, and API documentation |
| `3fc6541` | Fix vLLM Docker config for CUDA 12.3 and 8GB VRAM constraints |

---

## 12. README.md - docker-compose.yml 동기화 수정

### 사용자 프롬프트
> 작업 과정동안 변경된 내용이 README 파일의 내용과 차이나는 것은 없는지도 확인하고
> 앞으로 작업 과정중에 변경되는 모든 내용들은 해당 날짜의 타임스탬프로 시작하는 작업메모 파일에 기록하고 README 파일에도 기록하도록 해줘.

### 발견된 불일치 항목

| 항목 | README 기존값 | 실제 docker-compose.yml | 수정 |
|------|-------------|----------------------|------|
| `VLLM_GPU_MEMORY_UTILIZATION` 기본값 | 0.85 | 0.92 | ✅ 0.92로 수정 |
| `VLLM_MAX_MODEL_LEN` 기본값 | 4096 | 2048 | ✅ 2048로 수정 |
| vLLM 이미지 버전 정보 | 미기재 | v0.6.6.post1 | ✅ Docker Compose 섹션에 추가 |
| CUDA 호환성 안내 | Prerequisites에 추가 완료 | - | ✅ 이전 세션에서 완료 |

### 수정 내용
1. Configuration 테이블: `VLLM_GPU_MEMORY_UTILIZATION` 0.85 → 0.92, `VLLM_MAX_MODEL_LEN` 4096 → 2048
2. Docker Compose Commands 섹션: vLLM 이미지 버전(v0.6.6.post1) 및 CUDA 호환성 안내 Note 추가

### 앞으로의 규칙
- 모든 변경사항은 `docs/YYYYMMDD_work-memo.md` 작업메모 파일에 기록
- 동시에 `README.md`에도 반영하여 문서 동기화 유지

---

## 13. PDCA Gap Analysis 실행

### 사용자 프롬프트
> /pdca analyze backend

### 분석 결과
- **Overall Match Rate**: 93% (PASS)
- **보고서 위치**: `docs/03-analysis/backend.analysis.md`
- 총 118개 항목 분석, 108개 일치, 6개 부분 일치, 3개 누락, 5개 추가

### 발견된 주요 갭
| # | 심각도 | 항목 | 설명 |
|---|--------|------|------|
| 1 | Major | `POST /upload-folder` | 플랜에 있으나 미구현 |
| 2 | Minor | `.env.example` HF_TOKEN | docker-compose에서 사용하나 .env.example에 누락 |
| 3 | Minor | `.env.example` NEXT_PUBLIC_API_URL | frontend에서 사용하나 .env.example에 누락 |
| 4 | Minor | 기본값 불일치 | .env.example/config.py (0.85/4096) vs docker-compose/README (0.92/2048) |
| 5 | Minor | zustand 미사용 | package.json에 설치되었으나 코드에서 import 없음 |

---

## 14. PDCA Iterate - 갭 수정 (Act Phase)

### 사용자 프롬프트
> /pdca iterate backend

### 수정 내역

#### Fix 1: `.env.example` 기본값 동기화 + 누락 변수 추가
- `VLLM_GPU_MEMORY_UTILIZATION`: 0.85 → **0.92**
- `VLLM_MAX_MODEL_LEN`: 4096 → **2048**
- `HF_TOKEN=` 추가 (gated 모델용)
- `NEXT_PUBLIC_API_URL=http://localhost:8020` 추가 (frontend용)

#### Fix 2: `config.py` 기본값 동기화
- `vllm_gpu_memory_utilization`: 0.85 → **0.92**
- `vllm_max_model_len`: 4096 → **2048**

#### Fix 3: `POST /upload-folder` 엔드포인트 추가
- `backend/app/api/documents.py`에 `/upload-folder` 엔드포인트 추가
- 기존 `upload_documents()`를 내부 재사용하는 방식으로 구현

#### Fix 4: 미사용 `zustand` 의존성 제거
- `npm uninstall zustand` 실행
- `frontend/package.json`에서 제거됨

#### Fix 5: Frontend build 검증
- `npx next build` 성공 확인
